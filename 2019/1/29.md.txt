一個爬蟲和數據收集系統的設計

1.爬蟲部分去重隊列 ，重試機制

2.請求部分，多方式請求，rest，html,無GUI瀏覽器

3.存儲與清洗部分，唯一資源uuid，可結構化數據，json數據，不可結構化數據。

4,數據存儲之後的清洗，處理和展示部分。

https://scrapy-chs.readthedocs.io/zh_CN/0.24/topics/architecture.html


https://scrapy-chs.readthedocs.io/zh_CN/0.24/_images/scrapy_architecture.png

Scrapy是一个为了爬取网站数据、提取结构化数据而编写的爬虫应用框架。Scrapy内部实现了包括并发请求、免登录、URL去重等很多复杂操作，用户不需要明白Scrapy内部具体的爬取策略，只需要根据自己的需要，编写小部分的代码，就能抓取到所需要的数据。Scrapy主要由5个组成部分，若需要实现更多功能，Scrapy还提供了多种中间件。这五个模块及中间件的功能如下：

Scrapy Engine（Scrapy引擎）
Scrapy Engine是用来控制整个爬虫系统的数据处理流程，并进行不同事务触发。

Scheduler（调度器）
Scheduler维护着待爬取的URL队列，当调度程序从Scrapy Engine接受到请求时，会从待爬取的URL队列中取出下一个URL返还给他们。

Downloader（下载器）
Downloader从Scrapy Engine那里得到需要下载的URL，并向该网址发送网络请求进行页面网页，最后再将网页内容传递到Spiders来处理。如果需要定制更复杂的网络请求，可以通过Downloader中间件来实现，

Spiders（蜘蛛）
Spiders是用户需要编辑的代码的部分。用户通过编写spider.py这个类实现指定要爬取的网站地址、定义网址过滤规则、解析目标数据等。 Spider发出请求，并处理Scrapy Engine返回给它下载器响应数据，把解析到的数据以item的形式传递给Item Pipeline，把解析到的链接传递给Scheduler。

Item Pipeline（项目管道）
Item 定义了爬虫要抓取的数据的字段，类似于关系型数据库中表的字段名，用户编写item.py文件来实现这一功能。Pipeline主要负责处理Spider从网页中抽取的item，对item进行清洗、验证，并且将数据持久化，如将数据存入数据库或者文件。用户编写pipeline.py实现这一功能。

Downloader middlewares（下载器中间件）
Downloader middlewares是位于Scrapy Engine和Downloader之间的钩子框架，主要是处理Scrapy Engine与Downloader之间的请求及响应。可以代替接收请求、处理数据的下载以及将结果响应给Scrapy Engine。

Spider middlewares（蜘蛛中间件）
Spider middlewares是介于Scrapy Engine和Spiders之间的钩子框架，主要是处理Spiders的响应输入和请求输出。可以插入自定义的代码来处理发送给Spiders的请求和返回Spider获取的响应内容和项目。


Scrapy数据流是由执行的核心引擎(engine)控制，流程是这样的：

引擎打开一个域名，蜘蛛处理这个域名，并让蜘蛛获取第一个爬取的URL。

引擎从蜘蛛那获取第一个需要爬取的URL，然后作为请求在调度中进行调度。

引擎从调度那获取接下来进行爬取的页面。

调度将下一个爬取的URL返回给引擎，引擎将他们通过下载中间件发送到下载器。

当网页被下载器下载完成以后，响应内容通过下载中间件被发送到引擎。

引擎收到下载器的响应并将它通过蜘蛛中间件发送到蜘蛛进行处理。

蜘蛛处理响应并返回爬取到的item，然后给引擎发送新的请求。

引擎发送处理后的item到项目管道，然后把处理结果返回给调度器，调度器计划处理下一个请求抓取。

系统重复2-9的操作，直到调度中没有请求，然后断开引擎与域之间的联系。




$ hg clone http://hg.openjdk.java.net/loom/loom 
$ cd loom 
$ hg update -r fibers
$ sh configure  
$ make images